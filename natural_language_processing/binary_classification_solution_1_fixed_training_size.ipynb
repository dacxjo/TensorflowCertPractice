{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023, Troy Phat Tran (Mr. Troy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train a binary classifier for the language classification dataset. The dataset is typically a JSON array<br>\n",
    "of 500 JSON objects. Each object has 3 keys: sentence, language_code, and is_english.<br>\n",
    "We want our model to be able to determine whether a piece of text is \"English or not\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to fill in the missing parts of the code block (where commented as \"ADD CODE HERE\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the dataset is imbalanced as there are more non-English sentences than English ones. To keep things simple, <br>\n",
    "you don't need to handle data imbalance in this coding challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.src.callbacks import EarlyStopping\n",
    "from keras.src.layers import GlobalAveragePooling1D\n",
    "from keras.src.models import load_model\n",
    "from keras.src.preprocessing.text import Tokenizer\n",
    "from keras.src.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_binary_model():\n",
    "    # Download the dataset\n",
    "    json_file = 'language-classification.json'\n",
    "    if not os.path.exists(json_file):\n",
    "        url = 'https://trientran.github.io/tf-practice-exams/language-classification.json'\n",
    "        urlretrieve(url=url, filename=json_file)\n",
    "\n",
    "    # Parse the JSON file\n",
    "    with open(file=json_file, mode='r', encoding='utf-8') as f:\n",
    "        datastore = json.load(f)\n",
    "\n",
    "    # Extract texts and labels from JSON data\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for item in datastore:\n",
    "        texts.append(item['sentence'])  # replace with the sentence/paragraph/text field in the real test JSON file\n",
    "        labels.append(item['is_english'])  # replace with the label field in the real test JSON file\n",
    "\n",
    "    # Predefined constants\n",
    "    max_length = 25\n",
    "    trunc_type = 'pre'  # Can be replaced with 'post'\n",
    "    vocab_size = 500\n",
    "    padding_type = 'pre'  # Can be replaced with 'post'\n",
    "    embedding_dim = 32\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_size = 400\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    training_sentences = texts[0:training_size]\n",
    "    testing_sentences = texts[training_size:]\n",
    "    training_labels = labels[0:training_size]\n",
    "    validation_labels = labels[training_size:]\n",
    "\n",
    "    # Tokenize the texts\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "\n",
    "    # Pad the sequences\n",
    "    padded_training_set = pad_sequences(sequences=training_sequences,\n",
    "                                        maxlen=max_length,\n",
    "                                        padding=padding_type,\n",
    "                                        truncating=trunc_type)\n",
    "    padded_validation_set = pad_sequences(sequences=testing_sequences,\n",
    "                                          maxlen=max_length,\n",
    "                                          padding=padding_type,\n",
    "                                          truncating=trunc_type)\n",
    "\n",
    "    # Convert the labels to numpy array\n",
    "    training_labels = np.array(training_labels)\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        # ADD CODE HERE\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    # ADD CODE HERE\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Define an early stopping callback\n",
    "    # ADD CODE HERE\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Train the model\n",
    "    # ADD CODE HERE\n",
    "    model.fit(x=padded_training_set,\n",
    "              y=training_labels,\n",
    "              epochs=50,\n",
    "              validation_data=(padded_validation_set, validation_labels),\n",
    "              callbacks=[early_stop])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============DO NOT EDIT THIS PART================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Run and save your model\n",
    "    my_model = nlp_binary_model()\n",
    "    filepath = \"nlp_binary_model.h5\"\n",
    "    my_model.save(filepath)\n",
    "\n",
    "    # Reload the saved model\n",
    "    saved_model = load_model(filepath)\n",
    "\n",
    "    # Show the model architecture\n",
    "    saved_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
